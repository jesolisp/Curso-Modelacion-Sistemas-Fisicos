{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificación de sistemas dinámicos\n",
    "\n",
    "## Objetivo del problema de identificación.\n",
    "\n",
    "El objetivo de la Identificación de Sistemas (IS) consiste en construir modelos matemáticos de sistemas dinámicos a partir de datos experimentales. En la actualidad, la IS es importante en áreas como control automático, procesamiento de señales, física, economía, medicina, ecología, sismología, biología, entre otros.\n",
    "\n",
    "Gauss y Legendre fueron los primeros en introducir el método de mínimos cuadrados (LS) para predecir el movimiento de los planetas y cometas a partir de miediciones de telescopios. En consecuencia, diversos trabajos se realizaron sobre problemas de estimación de parámetros.\n",
    "\n",
    "Existen diferentes clases de modelos. Por ejemplo, modelos lineales deterministicos en tiempo discreto en representación de espacio de estados así como de la forma entrada-salida. También hay representaciones de sistemas a partir de modelos lineales estocásticos de tiempo discreto. Estos modelos pueden ser vistos como sigue:\n",
    "\n",
    "* Filtros dinámicos lineales que permitan la generación, análisis y clasificación de señales aleatorias. Por ejemplo, autorregresivas (AR), modelos promedio (MA) y modelos ARMA.\n",
    "* Modelos lineales con ruido aditivo aleatorio que son capaces de representar ruido, perturbaciones externas y errores de modelado. Por ejemplo, ARX, ARMAX y modelos ARARX.\n",
    "\n",
    "En la práctica, es común encontrar dos clases de modelos para representar sistemas del mundo real; los modelos no lineales (NL) y los modelos lineales de parámetros variables (LPV). \n",
    "\n",
    "Los modelos LPV son ideales para modelar sistemas lineales de tiempo variable (LTV) así como representar sistemas no lineales linealizados sobre una trayectoria $p(t)$. Este tipo de modelos pueden ser vistos como descripciones intermedias entre modelos lineales invariantes en el tiempo (LTI) y modelos no lineales variantes en el tiempo.\n",
    "\n",
    "Por otro lado, los modelos NL son muy utilizados para describir fenómenos o procesos complejos por ejemplo: procesos bioquímicos en columnas de destilación, plantad hidráulicas, sistemas fisiológicos, vibraciones en estructuras, por mencionar algunos.\n",
    "\n",
    "Los modelos NL orientados a bloques están compuestos por subsistemas dinámicos LTI y subsistemas estáticos NL donde la parte lineal son generalmente representaciones en funciones de transferencia, espacio de estados o I/O mientras que la parte NL pueden ser con memoria o sin memoria.\n",
    "\n",
    "El proceso para realizar la identificación de sistemas, según {cite:t}`ljung1998system` consisten en seis pasos:\n",
    "\n",
    "1. **Diseño del experimento.** Elección de la señal de excitación, periodo de muestreo, sensores para las señales de entrada y salida.\n",
    "2. **Mediciones de entrada y salida.** \n",
    "3. **Elección de la estructura del modelo.**\n",
    "4. **Determinación de la estructura de los parámetros.** Criterio de información de {cite:t}`akaike1974new` (AIC) y el criterio de longitud de descripción mínima de {cite:t}`rissanen1978modeling`.\n",
    "5. **Estimación paramétrica del modelo.**\n",
    "6. **Validación del modelo.** Medir el desempeño del modelo obtenido para representar los datos experimentales.\n",
    "\n",
    "La elección de los algoritmos para la estimación de parámetros depende de dos factores:\n",
    "\n",
    "1. La función costo a ser minimizada.\n",
    "2. El algoritmo de optimización para encontrar la solución óptima.\n",
    "\n",
    "De los métodos más utilizados para la estimación paramétrica podemos encontrar:\n",
    "\n",
    "* Método de mínimos cuadrados ponderado (WLS).\n",
    "    * Estimador Gauss-Markov.\n",
    "    * Mejor estimador lineal sin sesgo (BLUE).\n",
    "* Método de mínimos cuadrados generalizado (GLS).\n",
    "* Método de mínimos cuadrados extendido (ELS).\n",
    "* Método de mínimos cuadrados total (TLS).\n",
    "* Método de máxima verosimilitud (ML).\n",
    "* Método de máximo a posteriori (MAP).\n",
    "* Método de estimación de la mínima de la media del error al cuadrado (MMSE).\n",
    "* Método de estimación M de Huber.\n",
    "* Método de variable instrumental (IV).\n",
    "* Método de subespacio como el algoritmo MUSIC (MUltiple SIgnal Classification).\n",
    "\n",
    "## Variables aleatorias\n",
    "\n",
    "Asuma que el valor verdadero de una cantidad de datos es 20, donde el conjunto de datos está dado como sigue:\n",
    "\n",
    "|     1    |     2    |     3    |     4    |     5    |     6    |     7    |\n",
    "|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "| 20.13443 | 19.83828 | 20.01702 | 19.99835 | 19.94526 | 20.01415 | 19.96707 |\n",
    "\n",
    "En términos estadísticos, se puede decir que los datos anteriormente mostrados son generados por una *variable aleatoria* $X$ donde\n",
    "\n",
    "````{prf:definition} Variable aleatoria\n",
    " :label: variable-aleatoria\n",
    " Una variable aleatoria es una variable que tiene un valor numérico único, determinado por casualidad, para cada resultado de un procedimiento. \n",
    "````\n",
    "\n",
    "Suponga un sistema simple como un dado en el cual necesitamos conocer la probabilidad de obtener un \"3\" al lanzar el dado. La probabilidad de obtener este valor es 1/6 o bien, 16.7%. En estadística, podemos escribir esto como\n",
    "\n",
    "$$\n",
    " P(X_{1} = 3) = \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "donde $P$ es una función de probabilidad que representa la probabilidad del \"evento\" $X_{1} = 3$ como un número entre $0$ y $1$. Para tal efecto, definimos\n",
    "\n",
    "````{prf:definition} Eventos y espacio muestral\n",
    " :label: eventos-espacio\n",
    " 1. Un *evento* es cualquier colección de resultados o resultados de un procedimiento.\n",
    " 2. Un *evento simple* es un resultado o un evento que no se puede dividir en componentes más simples.\n",
    " 3. El *espacio muestral* para un procedimiento se conforma de todos los eventos simples posibles.\n",
    "````\n",
    "\n",
    "Por ejemplo, el espacio muestral para el ejemplo del dado está dado por el siguiente conjunto\n",
    "\n",
    "$$\n",
    " S = \\left\\{ 1, 2, 3, 4, 5, 6 \\right\\}\n",
    "$$\n",
    "y todos los subconjuntos $A \\subset S$ como $A_{1} = \\{1, 2\\}$ o $A_{2} = \\{1, 3, 5 \\}$ son eventos que pertenecen a un espacio muestral.\n",
    "\n",
    "La función de probabilidad $P$ parte de su definición en axiomas. Sin embargo, podemos definir probabilidad como sigue:\n",
    "\n",
    "````{prf:proposition} Probabilidad\n",
    " :label: probabilidad\n",
    " Dado un espacio muestral $S$, la *función de probabilidad* $P$ que asigna a cada evento $A \\subset $ un número $P(A) \\in [0,1]$, llamada *probabilidad\" del evento $A$, que dará una medida precisa de la posibilidad de que ocurra $A$.\n",
    "````\n",
    "\n",
    "````{prf:proposition} Enfoque clásico de probabilidad\n",
    " :label: probabilidad-clasica\n",
    " Un procedimiento dado tiene $n$ eventos simples diferentes y que cada uno de esos eventos simples tiene las mismas posibilidades de ocurrir. Si el evento $A$ puede ocurrir de estas $n$ maneras, entonces\n",
    "\n",
    " $$\n",
    "  P(A) = \\frac{s}{n}.\n",
    " $$\n",
    "````\n",
    "\n",
    "La {prf:proposition}`probabilidad-clasica` aplica para el ejemplo del dado así como para otras *variables discretas aleatorias* similares que involucran un número finito de resultados posibles igualmente probables. No obstante, no aplica para *variables aleatorias continuas* con un número infinito de posibles resultados similares a la variable aleatoria $X_{2}$.\n",
    "\n",
    "Para el espacio muestral dado por el conjunto\n",
    "\n",
    "$$\n",
    " S = \\left\\{ x \\in \\mathbb{R} | 0 \\leq x < 15 \\right\\}\n",
    "$$\n",
    "que involucra un número inifito de posibles resultados continuamente distribuidos entre 0 y 15, se puede utilizar la siguiente expresión\n",
    "\n",
    "````{prf:proposition} Aproximación de frecuencia relativa\n",
    " :label: aproximacion-relativa\n",
    " Asuma que dado que un procedimiento es repetido $n$ veces, y $f_{n}(A)$ denota la frecuencia relativa con la cual un evento $A$ ocurre, entonces\n",
    "\n",
    " $$\n",
    "  P(A) = \\lim_{n \\rightarrow \\infty} f_{n}(A)\n",
    " $$\n",
    "````\n",
    "\n",
    "## Representación de sistemas dinámicos en forma lineal con respecto a los parámetros.\n",
    "\n",
    "En los modelos de regresión (MR) se involucra el análisis de una variable dependiente en términos de una o más variables independientes. En estos modelos, los parámetros son ajustados para describir a los datos experimentales. Algunos autores consideran que los modelos de regresión son sistemas entrada-salida ya que contienen los elementos característicos de estos sistemas. Es decir, una entrada $x$, una salida $y$, un sistema tipo caja negra que transforma $x$ en $y$.\n",
    "\n",
    "### Modelo de regresión lineal\n",
    "\n",
    "Suponga que se tiene un conjunto de datos $\\left( x_{1}, y_{1} \\right), \\left( x_{2}, y_{2} \\right), \\dots, \\left( x_{m}, y_{m} \\right)$ $\\left( x_{i}, y_{i} \\in \\mathbb{R}, i=1, \\dots, m, ~ m \\in \\mathbb{N} \\right)$, entonces es posible describir datos experimentales utilizando una *función de regresión* o *función de modelo* de la forma\n",
    "\n",
    "$$\n",
    " \\hat{y}(x) = \\beta_{0}x + \\beta_{1},\n",
    "$$ (eqn:linear_regression)\n",
    "\n",
    "donde los coeficientes $\\beta_{0}$ y $\\beta_{1}$ son llamados *coeficientes de regresión* o *parámetros* del modelo de regresión. Además, $x$ es usualmente llamada *variable explicativa*, *variable predictora* o *variable independiente*, mientras que $\\hat{y}$ es conocida como *variable dependiente* o *variable respuesta*.\n",
    "\n",
    "```{note}\n",
    " La expresión dada en la Ec. {eq}`eqn:linear_regression` es llamada *función de regresión lineal* debido a que los coeficientes $\\beta_{0}$, $\\beta_{1}$ dependen linealmente de esta función.\n",
    "```\n",
    "\n",
    "Se dice que el modelo {eq}`eqn:linear_regression` ajusta los datos experimentales si la diferencia $y_{i}-\\hat{y}(x_{i})~(i=1,\\dots,m)$ es pequeña. Para lograr este objetivo, es necesario definir la siguiente expresión\n",
    "\n",
    "$$\n",
    " \\text{RSQ} = \\sum_{i=1}^{m} \\left( y_{i} - \\hat{y}\\left(x_{i}\\right) \\right)^{2}.\n",
    "$$\n",
    "\n",
    "Esta expresión es llamada *suma residual de cuadrados (RSQ)*. RSQ mide la distancia entre los datos experimentales y el modelo; en un problema de regresión estos parámetros pueden ser encontrados planteando un problema de optimización\n",
    "\n",
    "$$\n",
    " \\min_{\\beta_{0},\\beta_{1} \\in \\mathbb{R}} \\left\\{ \\text{RSQ} \\right\\}.\n",
    "$$\n",
    "\n",
    "La solución de este problema de optimización se puede obtener utilizando el procedimiento de minimización de una función de varias variables visto en cálculo de varias variables. Entonces, obteniendo las derivadas parciales de la función $RSQ(\\beta_{0},\\beta_{1})$ con respecto $\\beta_{0}$ y $\\beta_{1}$ a cero.\n",
    "\n",
    "$$\n",
    " \\beta_{0} = \\frac{\\sum_{i=1}^{m}~x_{i}y_{i} - m \\overline{x}\\overline{y}}{\\sum_{i=1}^{m}~x_{i}^{2} - m\\overline{x}^{2}},\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\beta_{1} = \\overline{y} - \\beta_{0} \\overline{x}\n",
    "$$\n",
    "\n",
    "```{note}\n",
    " RSQ es utilizado ya que realiza *estimaciones de máxima verosimilitud* de los parámetros del modelo, $\\beta_{0}$ y $\\beta_{1}$.\n",
    "```\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Considere los siguientes datos experimentales y encuentre un modelo de regresión lineal.\n",
    "\n",
    "|     | **1** | **2** | **3** | **4** | **5** |\n",
    "|-----|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| $x$ |   10  |   20  |   30  |   40  |   50  |\n",
    "| $y$ |   3   |   5   |   11  |   12  |   16  |\n",
    "\n",
    "```{note}\n",
    " Las funciones de regresión pueden ser utilizadas para predecir valores de la variable de respuesta a partir de las variables explicativas. Las buenas predicciones se alcanzan sólo si las funciones de regresión se ajustan lo suficientemente bien a los datos experimentales.\n",
    "```\n",
    "\n",
    "### Coeficiente de determinación\n",
    "\n",
    "El coeficiente de determinación mide la calidad de ajuste entre los datos experimentales y el modelo. Dicho valor se encuntra en una escala entre 0 y 100%, donde 0% representa un ajuste pobre mientras que el 100% implica un ajuste perfecto entre el modelo y los datos. El coeficiente de determinación está definido como\n",
    "\n",
    "$$\n",
    " R^{2} = \\frac{\\sum_{i=1}^{n}~ \\left( \\hat{y}_{i} - \\overline{y} \\right)^{2}}{\\sum_{i=1}^{n}~ \\left( y_{i} - \\overline{y} \\right)^{2}},\n",
    "$$\n",
    "\n",
    "donde $\\hat{y}_{i} = \\hat{y}\\left( x_{i} \\right)$. Para el caso de los modelos de regresión lineal, esto se puede rescribir como\n",
    "\n",
    "$$\n",
    " R^{2} = 1 - \\frac{\\sum_{i=1}^{n}~ \\left( \\hat{y}_{i} - \\overline{y} \\right)^{2}}{\\sum_{i=1}^{n}~ \\left( y_{i} - \\overline{y} \\right)^{2}},\n",
    "$$\n",
    "\n",
    "```{note}\n",
    " Si se obtienen valores negativos de $R^{2}$, entonces el modelo funciona peor que un modelo que produciría el valor medio.\n",
    "```\n",
    "\n",
    "### Modelo de regresión no lineal\n",
    "\n",
    "Debido a que algunos datos experimentales siguen algún patrón curvo no lineal que no se puede describir utilizando un modelo lineal, una alternativa es utilizar una *función de regresión polinomial* de la forma\n",
    "\n",
    "$$\n",
    " \\hat{y}(x) = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\cdots + \\beta_{s}x^{s}.\n",
    "$$ (eq:nonlinear-regression)\n",
    "\n",
    "Si bien la Ec. {eq}`eq:nonlinear-regression` representa una función que puede ser altamente no lineal, si $x$ es dado y además $\\hat{y}$ es calculado como una combinación lineal de los coeficientes de regresión $\\beta_{0},\\beta_{1},\\dots,\\beta_{s}$. Por lo tanto, todas las funciones de regresión se pueden llevar a la forma\n",
    "\n",
    "$$\n",
    " \\hat{y}(x) = \\beta_{0} + \\beta_{1}f_{1}(x) + \\beta_{2}f_{2}(x) + \\cdots + \\beta_{s}f_{s}(x).\n",
    "$$\n",
    "\n",
    "### Regresión lineal múltiple\n",
    "\n",
    "Suponga un problema en el que se desea predecir la variable de interés $y$ a partir de diversas variables independientes $x_{1}, x_{2}, \\dots, x_{n} \\left(n \\in \\mathbb{N}\\right)$. A este problema se le conoce como *problema de regresión lineal múltiple (MRLM)*. La expresión que representa al MRLM está dada como sigue\n",
    "\n",
    "````{prf:definition} Regresión múltiple\n",
    " Las funciones de regresión múltiple $\\hat{y}(\\mathrm{x})$ calculan una variable de respuesta $y$ a partir de variables explicativas $\\mathrm{x} = \\left( x_{1}, \\dots, x_{n} \\right)~ (n > 1)$ y los coeficientes de regresión $\\beta_{0}, \\beta_{1}, \\dots, \\beta_{s}$. Si $\\hat{y}(\\mathrm{x})$ depende linealmente de $\\beta_{0}, \\beta_{1}, \\dots, \\beta_{s}$ entonces los datos experimentales pueden ser ajustados utilizando un modelo de regresión lineal múltiple.\n",
    "````\n",
    "\n",
    "Entonces, la forma general de un modelo de regresión lineal múltiple que involucra un número arbitrario de $n\\in \\mathbb{N}$ variables explicatorias es \n",
    "\n",
    "$$\n",
    " \\hat{y}(\\mathrm{x}) = \\beta_{0} + \\beta_{1}f_{1}(\\mathrm{x}) + \\beta_{2}f_{1}(\\mathrm{x}) + \\cdots + \\beta_{s}f_{s}(\\mathrm{x}),\n",
    "$$\n",
    "\n",
    "donde $\\mathrm{x} = \\left( x_{1}, x_{2}, \\dots, x_{n} \\right)^{t}$ y $f_{i}$ son funciones reales arbitrarias.\n",
    "\n",
    "### Redes Neuronales Artificiales\n",
    "\n",
    "En los casos en los que la función de regresión no puede obtenerse de la teoría o en la que es difícil obtener una representación gráfica de los datos se pueden utilizar las redes neuronales artificiales (ANN). Este tipo de modelos pueden ser considerados como funciones de regresión no lineales con un gran número de parámetros a ajustar para aproximar a cualquier función suave.\n",
    "\n",
    "#### Redes neuronales de alimentación hacia adelante\n",
    "\n",
    "Este tipo de redes involucra una capa de entrada, una capa de salida y *capas ocultas* entre las capas de entrada y salida. En estos modelos se asume que la información viaja de izquierda a derecha y por esta razón reciben este nombre. Además, este tipo de representaciones matemáticas son suficientemente complejas para aproximar funciones suaves arbitrarias.\n",
    "\n",
    "Asuma que se tienen $n\\in \\mathbb{N}$ nodos de entrada que corresponden a las cantidades de entrada $x_{1},\\dots,x_{n}$, con $H \\in \\mathbb{N}$ nodos ocultos y $m \\in \\mathbb{N}$ nodos de salida que corresponden a las cantidades de salida $y_{1},\\dots,y_{m}$, si se multiplica cada entrada con una constante y se realiza la suma de todas las entradas y se agrega una constante se obtiene la siguiente expresión\n",
    "\n",
    "$$\n",
    " \\sum_{k=1}^{n} w_{ik;h1}x_{k} + b_{h1},\n",
    "$$ (eq:linear-kernel)\n",
    "\n",
    "donde los llamados *pesos* $w_{ik;h_{1}}$ representan el coeficiente real utilizado por el nodo oculto para multiplicar a la $k$-ésima entrada ($ik$) mientras que los sesgos o \"bias\" $b_{h_{1}}$ son sumados por el nodo oculto.\n",
    "\n",
    "Para evitar que la expresión {eq}`eq:linear-kernel` sea considerada como una forma compleja de un modelo de regresión lineal múltiple es necesario que en el nodo oculto exista una función real no lineal  llamada *función de activación*\n",
    "\n",
    "$$\n",
    " \\varphi_{h} \\left( \\sum_{k=1}^{n} w_{ik;h1}x_{k} + b_{h1} \\right).\n",
    "$$ (eq:single-ann)\n",
    "\n",
    "La función de activación más utilizada es la función logística \n",
    "\n",
    "$$\n",
    " \\varphi(\\xi) = \\frac{e^{\\xi}}{1 + e^{\\xi}},\n",
    "$$ (eq:logistic-function)\n",
    "\n",
    "donde el estado de los nodos ocultos $l=1,\\dots,H$ después del procesamiento de las entradas se puede representar como sigue\n",
    "\n",
    "$$\n",
    " \\varphi_{h} \\left( \\sum_{k=1}^{n} w_{ik;hl} \\cdot x_{k} + b_{hl} \\right), \\quad l=1,\\dots,H.\n",
    "$$ (eq:single-ann-n)\n",
    "\n",
    "Asumiendo que la capa de salida procesa esta entrada de la misma manera que la capa oculta utilizando diferentes coeficientes y una función no lineal diferente, es posible obtener una salida a partir de \n",
    "\n",
    "$$\n",
    " y_{j} = \\varphi_{o}\\left( b_{oj} + \\sum_{k=1}^{n} w_{ik,oj} \\cdot x_{k} + \\sum_{l=1}^{H} w_{hl,oj} \\cdot \\varphi_{h} \\left( \\sum_{k=1}^{n} w_{ik;hl} \\cdot x_{k} + b_{hl} \\right) \\right), \\quad j=1,\\dots,m.\n",
    "$$\n",
    "\n",
    "#### Máquina de aprendizaje extrema\n",
    "\n",
    "De acuerdo con {cite:t}`huang2006extreme`, los algoritmos de entrenamiento para redes neuronales son lentos y pueden llevar horas, días o inclusive más tiempo para encontrar los parámetros del modelo.\n",
    "\n",
    "En la práctica, las redes neuronales se entrenan en un conjunto de entrenamiento finito. Partiendo de esta suposición, {cite:t}`huang1998upper` mostraron que una red neuronal de retroalimentación (SLFN) con una capa oculta con máximo $N$ nodos ocultos y casi cualquier función de activación no lineal puede aprender exactamente $N$ observaciones distintas.\n",
    "\n",
    "Para $N$ muestras distintas arbitrarias $\\left( \\mathrm{x}_{i}, \\mathrm{t}_{i} \\right)$ donde $\\mathrm{x}_{i} = \\begin{bmatrix} x_{i1},x_{i2},\\dots,x_{in} \\end{bmatrix}^{T} \\in \\mathbb{R}^{n}$ y $\\mathrm{t}_{i} = \\begin{bmatrix} t_{i1},t_{i2},\\dots,t_{im} \\end{bmatrix}^{T} \\in \\mathbb{R}^{m}$, la red feedforward de capa oculta única (SLFN) con nodos ocultos aleatorios y función de activación $\\varphi$ se modelada matemáticamente como \n",
    "\n",
    "$$\n",
    " \\sum_{i=1}^{Ñ}\\beta_{i}\\varphi_{i}(\\mathrm{x}_{j}) = \\sum_{i=1}^{Ñ}\\beta_{i}\\varphi_{i}(\\mathrm{w}_{i} \\cdot \\mathrm{x}_{j} + b_{i}) = \\mathrm{o}_{j}, \\quad j=1,\\dots,N,\n",
    "$$\n",
    "donde $\\mathrm{w}_{i} = \\begin{bmatrix} w_{i1}, w_{i2}, \\dots, w_{in} \\end{bmatrix}^{T}$ es el vector de psos conectados al $i$-ésimo nodo oculto y los nodos de entrada, $\\beta_{i} = \\begin{bmatrix} \\beta_{i1}, \\beta_{i2}, \\dots, \\beta_{im} \\end{bmatrix}^{T}$ es el vector de pesos conectados al $i$-ésimo nodo oculto y los nodos de salida, además $b_{i}$ es el umbral de el $i$-ésimo nodo oculto. \n",
    "\n",
    "Entonces, la red SLFN con $Ñ$ nodos ocultos y función de activación $\\varphi$ puede aproximar las $N$ muestras con cero error $\\sum_{j=1}^{Ñ} ||\\mathrm{o}_{j} - \\mathrm{t}_{j}|| = 0 $, i.e. que existe $\\beta_{i}$, $\\mathrm{w}_{i}$ y $b_{i}$ tal que \n",
    "\n",
    "$$\n",
    " \\sum_{i=1}^{Ñ}\\beta_{i}\\varphi_{i}(\\mathrm{w}_{i} \\cdot \\mathrm{x}_{j} + b_{i}) = \\mathrm{t}_{j}, \\quad j=1,\\dots, N.\n",
    "$$\n",
    "\n",
    "Las $N$ ecuaciones se pueden rescribir como\n",
    "\n",
    "$$\n",
    " \\mathrm{H}\\beta = \\mathrm{T},\n",
    "$$\n",
    "donde\n",
    "\n",
    "$$\n",
    " \\mathrm{H}\\left(\\mathrm{w}_{1},\\dots,\\mathrm{w}_{Ñ},b_{1},\\dots,b_{Ñ},\\mathrm{x}_{1},\\dots,\\mathrm{x}_{N} \\right) = \\begin{bmatrix} \\varphi \\left( \\mathrm{w}_{1} \\cdot \\mathrm{x}_{1} + b_{1} \\right) & \\cdots & \\varphi \\left( \\mathrm{w}_{Ñ} \\cdot \\mathrm{x}_{1} + b_{Ñ} \\right) \\\\ \\vdots & \\cdots & \\vdots \\\\ \\varphi \\left( \\mathrm{w}_{1} \\cdot \\mathrm{x}_{N} + b_{1} \\right) & \\cdots & \\varphi \\left( \\mathrm{w}_{Ñ} \\cdot \\mathrm{x}_{N} + b_{Ñ} \\right) \\end{bmatrix}_{N\\times Ñ},\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\beta = \\begin{bmatrix} \\beta_{1}^{T} \\\\ \\vdots \\\\ \\beta_{Ñ}^{T} \\end{bmatrix}_{Ñ\\times m}, \\quad \\mathrm{T} = \\begin{bmatrix} t_{1}^{T} \\\\ \\vdots \\\\ t_{Ñ}^{T} \\end{bmatrix}_{N\\times m}.\n",
    "$$\n",
    "\n",
    "Para pesos de entrada fijos $\\mathrm{w}_{i}$ y sesgos de capa oculta $b_{i}$, el entrenamiento de una red tipo SLFN es equivalente a encontrar una solución de mínimos cuadrados $\\hat{\\beta}$ del sistema lineal $\\mathrm{H}\\beta = \\mathrm{T}$\n",
    "\n",
    "$$\n",
    " \\lVert  H \\left( \\mathrm{w}_{1},\\dots,\\mathrm{w}_{Ñ},b_{1},\\dots,b_{Ñ} \\right) \\hat{\\beta} - \\mathrm{T} \\lVert = \\min_{\\beta} \\lVert H \\left( \\mathrm{w}_{1},\\dots,\\mathrm{w}_{Ñ},b_{1},\\dots,b_{Ñ} \\right) {\\beta} - \\mathrm{T} \\lVert\n",
    "$$\n",
    "\n",
    "Si $Ñ=$$, entonces la matrix $\\mathrm{H}$ es cuadrada y por consiguiente tiene inversa cuando el vector de peso de entrada $\\mathrm{w}_{i}$ y los sesgos ocultos $b_{i}$ son escogidos aleatoriamente.\n",
    "\n",
    "En la práctica, $Ñ \\ll N$. Por lo tanto, $\\mathrm{H}$ es rectangular y podría no existir $\\mathrm{w}_{i}, b_{i}, \\beta_{i}~(i=1,\\dots,Ñ)$ tal que $\\mathrm{H}\\beta \\mathrm{T}$. Por consiguiente, la solución del sistema lineal está dada como sigue\n",
    "\n",
    "$$\n",
    " \\hat{\\beta} = \\mathrm{H}^{\\dagger}\\mathrm{T},\n",
    "$$\n",
    "\n",
    "donde $\\mathrm{H}^{\\dagger}$ es la *inversa generalizada Moore-Penrose* de la matriz $\\mathrm{H}$ la cual satisface las siguientes condiciones:\n",
    "\n",
    "1. $\\mathrm{H} \\mathrm{H}^{\\dagger} \\mathrm{H} = \\mathrm{H}$,\n",
    "2. $\\mathrm{H}^{\\dagger} \\mathrm{H} \\mathrm{H}^{\\dagger} = \\mathrm{H}^{\\dagger}$,\n",
    "3. $\\left( \\mathrm{H} \\mathrm{H}^{\\dagger} \\right)^{T} = \\mathrm{H} \\mathrm{H}^{\\dagger}$,\n",
    "4. $\\left( \\mathrm{H}^{\\dagger} \\mathrm{H} \\right)^{T} = \\mathrm{H}^{\\dagger} \\mathrm{H}$.\n",
    "\n",
    "```{note}\n",
    " En el caso especial cuando la matriz $\\mathrm{H}$ es una matriz cuadrada no singular, la pseudoinversa de $\\mathrm{H}$ es simplemente su inversa, i.e. $\\mathrm{H}^{\\dagger} = \\mathrm{H}^{-1}$.\n",
    "```\n",
    "La pseudoinversa también está definida como\n",
    "\n",
    "$$\n",
    " \\mathrm{H}^{\\dagger} = \\lim_{\\nu \\rightarrow 0} \\left( \\mathrm{H}^{T}\\mathrm{H} + \\nu^{2} I \\right)^{-1}\\mathrm{H}^{T} = \\lim_{\\nu \\rightarrow 0} \\mathrm{H}^{T} \\left( \\mathrm{H} \\mathrm{H}^{T} + \\nu^{2}I \\right)^{-1}\n",
    "$$\n",
    "\n",
    "o bien\n",
    "\n",
    "$$\n",
    " \\mathrm{H}^{\\dagger} = \\left( \\mathrm{H}^{T} \\mathrm{H} \\right)^{-1}\\mathrm{H}^{T} = \\mathrm{H}^{T} \\left( \\mathrm{H} \\mathrm{H}^{T} \\right)^{-1},\n",
    "$$\n",
    "\n",
    "si $\\mathrm{H}^{T}\\mathrm{H}$ o $\\mathrm{H} \\mathrm{H}^{T}$ es no singular.\n",
    "\n",
    "Finalmente, el método de entrenamiento para la SLFN llamado máquina de aprendizaje extrema (ELM) se presenta a continuación:\n",
    "\n",
    "## Métodos de estimación de parámetros.\n",
    "\n",
    "## Diseño de experimentos para la identificación paramétrica.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
